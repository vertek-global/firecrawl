{"version":3,"file":"crawler.test.js","sourceRoot":"","sources":["../../../../../src/scraper/WebScraper/__tests__/crawler.test.ts"],"names":[],"mappings":";;;;;AAAA,kBAAkB;AAClB,wCAAwC;AACxC,kDAA0B;AAC1B,kEAAyC;AAEzC,IAAI,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC;AACnB,IAAI,CAAC,IAAI,CAAC,eAAe,CAAC,CAAC;AAE3B,QAAQ,CAAC,YAAY,EAAE,GAAG,EAAE;IAC1B,IAAI,OAAmB,CAAC;IACxB,MAAM,SAAS,GAAG,eAAkC,CAAC;IACrD,MAAM,gBAAgB,GAAG,uBAExB,CAAC;IAEF,IAAI,eAAuB,CAAC;IAE5B,UAAU,CAAC,GAAG,EAAE;QACd,sBAAsB;QACtB,SAAS,CAAC,GAAG,CAAC,kBAAkB,CAAC,CAAC,GAAG,EAAE,EAAE;YACvC,IAAI,GAAG,CAAC,QAAQ,CAAC,YAAY,CAAC,EAAE,CAAC;gBAC/B,OAAO,OAAO,CAAC,OAAO,CAAC,EAAE,IAAI,EAAE,yBAAyB,EAAE,CAAC,CAAC;YAC9D,CAAC;iBAAM,IAAI,GAAG,CAAC,QAAQ,CAAC,aAAa,CAAC,EAAE,CAAC;gBACvC,OAAO,OAAO,CAAC,OAAO,CAAC,EAAE,IAAI,EAAE,iBAAiB,EAAE,CAAC,CAAC,CAAC,wCAAwC;YAC/F,CAAC;YACD,OAAO,OAAO,CAAC,OAAO,CAAC,EAAE,IAAI,EAAE,eAAe,EAAE,CAAC,CAAC;QACpD,CAAC,CAAC,CAAC;QAEH,gBAAgB,CAAC,eAAe,CAAC;YAC/B,SAAS,EAAE,IAAI,CAAC,EAAE,EAAE,CAAC,eAAe,CAAC,IAAI,CAAC;YAC1C,YAAY,EAAE,IAAI,CAAC,EAAE,EAAE,CAAC,eAAe,CAAC,KAAK,CAAC;YAC9C,qBAAqB,EAAE,IAAI,CAAC,EAAE,EAAE,CAAC,eAAe,CAAC,CAAC,CAAC;YACnD,aAAa,EAAE,IAAI,CAAC,EAAE,EAAE,CAAC,eAAe,CAAC,CAAC,CAAC;YAC3C,WAAW,EAAE,IAAI,CAAC,EAAE,EAAE,CAAC,eAAe,CAAC,EAAE,CAAC;YAC1C,gBAAgB,EAAE,IAAI,CAAC,EAAE,EAAE,CAAC,eAAe,CAAC,aAAa,CAAC;SAC3D,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,+EAA+E,EAAE,KAAK,IAAI,EAAE;QAC7F,MAAM,UAAU,GAAG,oBAAoB,CAAC;QACxC,MAAM,KAAK,GAAG,CAAC,CAAC,CAAC,sCAAsC;QAEvD,OAAO,GAAG,IAAI,oBAAU,CAAC;YACvB,KAAK,EAAE,MAAM;YACb,UAAU,EAAE,UAAU;YACtB,QAAQ,EAAE,EAAE;YACZ,QAAQ,EAAE,EAAE;YACZ,KAAK,EAAE,KAAK,EAAE,kBAAkB;YAChC,eAAe,EAAE,EAAE;SACpB,CAAC,CAAC;QAEH,qEAAqE;QACrE,OAAO,CAAC,sBAAsB,CAAC,GAAG,IAAI;aACnC,EAAE,EAAE;aACJ,iBAAiB,CAAC;YACjB,UAAU;YACV,UAAU,GAAG,QAAQ;YACrB,UAAU,GAAG,QAAQ;YACrB,UAAU,GAAG,QAAQ;SACtB,CAAC,CAAC;QAEL,MAAM,aAAa,GAAG,OAAO,CAAC,aAAa,CAAC,CAC1C;YACE,UAAU;YACV,UAAU,GAAG,QAAQ;YACrB,UAAU,GAAG,QAAQ;YACrB,UAAU,GAAG,QAAQ;SACtB,EACD,KAAK,EACL,EAAE,CACH,CAAC;QAEF,MAAM,CAAC,aAAa,CAAC,KAAK,CAAC,MAAM,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC,oDAAoD;QACpG,MAAM,CAAC,aAAa,CAAC,KAAK,CAAC,CAAC,OAAO,CAAC,CAAC,UAAU,EAAE,UAAU,GAAG,QAAQ,CAAC,CAAC,CAAC;IAC3E,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC","sourcesContent":["// crawler.test.ts\nimport { WebCrawler } from \"../crawler\";\nimport axios from \"axios\";\nimport robotsParser from \"robots-parser\";\n\njest.mock(\"axios\");\njest.mock(\"robots-parser\");\n\ndescribe(\"WebCrawler\", () => {\n  let crawler: WebCrawler;\n  const mockAxios = axios as jest.Mocked<typeof axios>;\n  const mockRobotsParser = robotsParser as jest.MockedFunction<\n    typeof robotsParser\n  >;\n\n  let maxCrawledDepth: number;\n\n  beforeEach(() => {\n    // Setup default mocks\n    mockAxios.get.mockImplementation((url) => {\n      if (url.includes(\"robots.txt\")) {\n        return Promise.resolve({ data: \"User-agent: *\\nAllow: /\" });\n      } else if (url.includes(\"sitemap.xml\")) {\n        return Promise.resolve({ data: \"sitemap content\" }); // You would normally parse this to URLs\n      }\n      return Promise.resolve({ data: \"<html></html>\" });\n    });\n\n    mockRobotsParser.mockReturnValue({\n      isAllowed: jest.fn().mockReturnValue(true),\n      isDisallowed: jest.fn().mockReturnValue(false),\n      getMatchingLineNumber: jest.fn().mockReturnValue(0),\n      getCrawlDelay: jest.fn().mockReturnValue(0),\n      getSitemaps: jest.fn().mockReturnValue([]),\n      getPreferredHost: jest.fn().mockReturnValue(\"example.com\"),\n    });\n  });\n\n  it(\"should respect the limit parameter by not returning more links than specified\", async () => {\n    const initialUrl = \"http://example.com\";\n    const limit = 2; // Set a limit for the number of links\n\n    crawler = new WebCrawler({\n      jobId: \"TEST\",\n      initialUrl: initialUrl,\n      includes: [],\n      excludes: [],\n      limit: limit, // Apply the limit\n      maxCrawledDepth: 10,\n    });\n\n    // Mock sitemap fetching function to return more links than the limit\n    crawler[\"tryFetchSitemapLinks\"] = jest\n      .fn()\n      .mockResolvedValue([\n        initialUrl,\n        initialUrl + \"/page1\",\n        initialUrl + \"/page2\",\n        initialUrl + \"/page3\",\n      ]);\n\n    const filteredLinks = crawler[\"filterLinks\"](\n      [\n        initialUrl,\n        initialUrl + \"/page1\",\n        initialUrl + \"/page2\",\n        initialUrl + \"/page3\",\n      ],\n      limit,\n      10,\n    );\n\n    expect(filteredLinks.links.length).toBe(limit); // Check if the number of results respects the limit\n    expect(filteredLinks.links).toEqual([initialUrl, initialUrl + \"/page1\"]);\n  });\n});\n"]}