{"version":3,"file":"types.js","sourceRoot":"","sources":["../../src/types.ts"],"names":[],"mappings":";;;AAyJA,IAAY,eAWX;AAXD,WAAY,eAAe;IACzB,kCAAe,CAAA;IACf,8CAA2B,CAAA;IAC3B,oCAAiB,CAAA;IACjB,4DAAyC,CAAA;IACzC,sCAAmB,CAAA;IACnB,oCAAiB,CAAA;IACjB,8BAAW,CAAA;IACX,sCAAmB,CAAA;IACnB,kDAA+B,CAAA;IAC/B,8DAA2C,CAAA;AAC7C,CAAC,EAXW,eAAe,+BAAf,eAAe,QAW1B;AAeD,IAAY,gBAQX;AARD,WAAY,gBAAgB;IAC1B,0DAAsC,CAAA;IACtC,kDAA8B,CAAA;IAC9B,2DAAuC,CAAA;IACvC,iEAA6C,CAAA;IAC7C,+DAA2C,CAAA;IAC3C,yEAAqD,CAAA;IACrD,mEAA+C,CAAA;AACjD,CAAC,EARW,gBAAgB,gCAAhB,gBAAgB,QAQ3B","sourcesContent":["import { z } from \"zod\";\nimport {\n  AuthCreditUsageChunk,\n  BaseScrapeOptions,\n  ScrapeOptions,\n  Document as V1Document,\n  webhookSchema,\n} from \"./controllers/v1/types\";\nimport { ExtractorOptions, Document } from \"./lib/entities\";\nimport { InternalOptions } from \"./scraper/scrapeURL\";\nimport type { CostTracking } from \"./lib/extract/extraction-service\";\n\ntype Mode = \"crawl\" | \"single_urls\" | \"sitemap\" | \"kickoff\";\n\nexport { Mode };\n\nexport interface CrawlResult {\n  source: string;\n  content: string;\n  options?: {\n    summarize?: boolean;\n    summarize_max_chars?: number;\n  };\n  metadata?: any;\n  raw_context_id?: number | string;\n  permissions?: any[];\n}\n\nexport interface IngestResult {\n  success: boolean;\n  error: string;\n  data: CrawlResult[];\n}\n\nexport interface WebScraperOptions {\n  url: string;\n  mode: Mode;\n  crawlerOptions?: any;\n  scrapeOptions: BaseScrapeOptions;\n  internalOptions?: InternalOptions;\n  team_id: string;\n  origin?: string;\n  crawl_id?: string;\n  sitemapped?: boolean;\n  webhook?: z.infer<typeof webhookSchema>;\n  v1?: boolean;\n  integration?: string | null;\n\n  /**\n   * Disables billing on the worker side.\n   */\n  is_scrape?: boolean;\n\n  isCrawlSourceScrape?: boolean;\n  from_extract?: boolean;\n  startTime?: number;\n\n  zeroDataRetention: boolean;\n  sentry?: any;\n  is_extract?: boolean;\n  concurrencyLimited?: boolean;\n}\n\nexport interface RunWebScraperParams {\n  url: string;\n  mode: Mode;\n  scrapeOptions: ScrapeOptions;\n  internalOptions?: InternalOptions;\n  // onSuccess: (result: V1Document, mode: string) => void;\n  // onError: (error: Error) => void;\n  team_id: string;\n  bull_job_id: string;\n  priority?: number;\n  is_scrape?: boolean;\n  is_crawl?: boolean;\n  urlInvisibleInCurrentCrawl?: boolean;\n  costTracking: CostTracking;\n}\n\nexport type RunWebScraperResult =\n  | {\n      success: false;\n      error: Error;\n    }\n  | {\n      success: true;\n      document: V1Document;\n    };\n\nexport interface FirecrawlJob {\n  job_id?: string;\n  success: boolean;\n  message?: string;\n  num_docs: number;\n  docs: any[];\n  time_taken: number;\n  team_id: string;\n  mode: string;\n  url: string;\n  crawlerOptions?: any;\n  scrapeOptions?: any;\n  origin: string;\n  integration?: string | null;\n  num_tokens?: number;\n  retry?: boolean;\n  crawl_id?: string;\n  tokens_billed?: number;\n  sources?: Record<string, string[]>;\n  cost_tracking?: CostTracking;\n  pdf_num_pages?: number;\n  credits_billed?: number | null;\n  change_tracking_tag?: string | null;\n  dr_clean_by?: string | null;\n\n  zeroDataRetention: boolean;\n}\n\nexport interface FirecrawlScrapeResponse {\n  statusCode: number;\n  body: {\n    status: string;\n    data: Document;\n  };\n  error?: string;\n}\n\nexport interface FirecrawlCrawlResponse {\n  statusCode: number;\n  body: {\n    status: string;\n    jobId: string;\n  };\n  error?: string;\n}\n\nexport interface FirecrawlCrawlStatusResponse {\n  statusCode: number;\n  body: {\n    status: string;\n    data: Document[];\n  };\n  error?: string;\n}\n\nexport interface FirecrawlExtractResponse {\n  statusCode: number;\n  body: {\n    success: boolean;\n    data: any[];\n  };\n  error?: string;\n}\n\nexport enum RateLimiterMode {\n  Crawl = \"crawl\",\n  CrawlStatus = \"crawlStatus\",\n  Scrape = \"scrape\",\n  ScrapeAgentPreview = \"scrapeAgentPreview\",\n  Preview = \"preview\",\n  Search = \"search\",\n  Map = \"map\",\n  Extract = \"extract\",\n  ExtractStatus = \"extractStatus\",\n  ExtractAgentPreview = \"extractAgentPreview\",\n}\n\nexport type AuthResponse =\n  | {\n      success: true;\n      team_id: string;\n      api_key?: string;\n      chunk: AuthCreditUsageChunk | null;\n    }\n  | {\n      success: false;\n      error: string;\n      status: number;\n    };\n\nexport enum NotificationType {\n  APPROACHING_LIMIT = \"approachingLimit\",\n  LIMIT_REACHED = \"limitReached\",\n  RATE_LIMIT_REACHED = \"rateLimitReached\",\n  AUTO_RECHARGE_SUCCESS = \"autoRechargeSuccess\",\n  AUTO_RECHARGE_FAILED = \"autoRechargeFailed\",\n  CONCURRENCY_LIMIT_REACHED = \"concurrencyLimitReached\",\n  AUTO_RECHARGE_FREQUENT = \"autoRechargeFrequent\",\n}\n\nexport type ScrapeLog = {\n  url: string;\n  scraper: string;\n  success?: boolean;\n  response_code?: number;\n  time_taken_seconds?: number;\n  proxy?: string;\n  retried?: boolean;\n  error_message?: string;\n  date_added?: string; // ISO 8601 format\n  html?: string;\n  ipv4_support?: boolean | null;\n  ipv6_support?: boolean | null;\n};\n\nexport type WebhookEventType =\n  | \"crawl.page\"\n  | \"batch_scrape.page\"\n  | \"crawl.started\"\n  | \"batch_scrape.started\"\n  | \"crawl.completed\"\n  | \"batch_scrape.completed\"\n  | \"crawl.failed\";\n"]}