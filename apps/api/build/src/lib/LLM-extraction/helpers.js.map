{"version":3,"file":"helpers.js","sourceRoot":"","sources":["../../../../src/lib/LLM-extraction/helpers.ts"],"names":[],"mappings":";;AAIA,kDAiBC;AArBD,6CAAoD;AAGpD,2FAA2F;AAC3F,SAAgB,mBAAmB,CAAC,OAAe,EAAE,KAAa;IAChE,MAAM,OAAO,GAAG,IAAA,6BAAkB,EAAC,KAAsB,CAAC,CAAC;IAE3D,iCAAiC;IACjC,IAAI,MAAmB,CAAC;IACxB,IAAI,CAAC;QACH,MAAM,GAAG,OAAO,CAAC,MAAM,CAAC,OAAO,CAAC,CAAC;IACnC,CAAC;IAAC,OAAO,KAAK,EAAE,CAAC;QACf,OAAO,GAAG,OAAO,CAAC,OAAO,CAAC,eAAe,EAAE,EAAE,CAAC,CAAC;QAC/C,MAAM,GAAG,OAAO,CAAC,MAAM,CAAC,OAAO,CAAC,CAAC;IACnC,CAAC;IAED,uCAAuC;IACvC,OAAO,CAAC,IAAI,EAAE,CAAC;IAEf,8BAA8B;IAC9B,OAAO,MAAM,CAAC,MAAM,CAAC;AACvB,CAAC","sourcesContent":["import { encoding_for_model } from \"@dqbd/tiktoken\";\nimport { TiktokenModel } from \"@dqbd/tiktoken\";\n\n// This function calculates the number of tokens in a text string using GPT-3.5-turbo model\nexport function numTokensFromString(message: string, model: string): number {\n  const encoder = encoding_for_model(model as TiktokenModel);\n\n  // Encode the message into tokens\n  let tokens: Uint32Array;\n  try {\n    tokens = encoder.encode(message);\n  } catch (error) {\n    message = message.replace(\"<|endoftext|>\", \"\");\n    tokens = encoder.encode(message);\n  }\n\n  // Free the encoder resources after use\n  encoder.free();\n\n  // Return the number of tokens\n  return tokens.length;\n}\n"]}