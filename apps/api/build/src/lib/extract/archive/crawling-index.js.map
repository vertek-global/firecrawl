{"version":3,"file":"crawling-index.js","sourceRoot":"","sources":["../../../../../src/lib/extract/archive/crawling-index.ts"],"names":[],"mappings":";AAAA,kCAAkC;;AAElC,4BAA4B;AAC5B,iDAAiD;AACjD,6CAA6C;AAC7C,oBAAoB;AACpB,mBAAmB;AACnB,wBAAwB;AACxB,wBAAwB;AACxB,4BAA4B;AAC5B,iCAAiC;AACjC,gCAAgC;AAChC,8BAA8B;AAC9B,8BAA8B;AAC9B,qCAAqC;AACrC,mCAAmC;AACnC,QAAQ;AACR,qBAAqB;AACrB,+BAA+B;AAC/B,+BAA+B;AAC/B,oBAAoB;AACpB,uBAAuB;AACvB,kCAAkC;AAClC,yBAAyB;AACzB,wBAAwB;AACxB,oCAAoC;AACpC,OAAO;AACP,uBAAuB;AACvB,mCAAmC;AACnC,8BAA8B;AAC9B,OAAO;AACP,oDAAoD;AACpD,2BAA2B;AAC3B,gDAAgD;AAChD,KAAK;AAEL,kCAAkC;AAClC,2BAA2B;AAE3B,2BAA2B;AAC3B,gCAAgC;AAChC,2CAA2C;AAC3C,8BAA8B;AAC9B,oDAAoD;AACpD,gDAAgD;AAChD,uCAAuC;AACvC,qCAAqC;AACrC,yCAAyC;AACzC,qBAAqB;AACrB,kBAAkB;AAClB,mBAAmB;AACnB,cAAc;AACd,mCAAmC;AAEnC,0EAA0E;AAC1E,wEAAwE;AACxE,iFAAiF;AAEjF,8BAA8B;AAC9B,yCAAyC;AACzC,0EAA0E;AAC1E,iBAAiB;AACjB,aAAa;AACb,WAAW;AACX,0BAA0B;AAC1B,uDAAuD;AACvD,OAAO;AACP,sBAAsB;AACtB,sBAAsB;AACtB,oBAAoB;AACpB,6BAA6B;AAC7B,2BAA2B;AAC3B,WAAW;AACX,gBAAgB;AAChB,0BAA0B;AAC1B,iDAAiD;AACjD,WAAW;AACX,SAAS;AACT,OAAO;AACP,oCAAoC;AACpC,IAAI","sourcesContent":["// const id = crypto.randomUUID();\n\n// const sc: StoredCrawl = {\n//   originUrl: request.urls[0].replace(\"/*\",\"\"),\n//   crawlerOptions: toLegacyCrawlerOptions({\n//     maxDepth: 15,\n//     limit: 5000,\n//     includePaths: [],\n//     excludePaths: [],\n//     ignoreSitemap: false,\n//     allowExternalLinks: false,\n//     allowBackwardLinks: true,\n//     allowSubdomains: false,\n//     ignoreRobotsTxt: false,\n//     deduplicateSimilarURLs: false,\n//     ignoreQueryParameters: false\n//   }),\n//   scrapeOptions: {\n//       formats: [\"markdown\"],\n//       onlyMainContent: true,\n//       waitFor: 0,\n//       mobile: false,\n//       removeBase64Images: true,\n//       fastMode: false,\n//       parsePDF: true,\n//       skipTlsVerification: false,\n//   },\n//   internalOptions: {\n//     disableSmartWaitCache: true,\n//     isBackgroundIndex: true\n//   },\n//   team_id: process.env.BACKGROUND_INDEX_TEAM_ID!,\n//   createdAt: Date.now(),\n//   plan: \"hobby\", // make it a low concurrency\n// };\n\n// // Save the crawl configuration\n// await saveCrawl(id, sc);\n\n// // Then kick off the job\n// await _addScrapeJobToBullMQ({\n//   url: request.urls[0].replace(\"/*\",\"\"),\n//   mode: \"kickoff\" as const,\n//   team_id: process.env.BACKGROUND_INDEX_TEAM_ID!,\n//   plan: \"hobby\", // make it a low concurrency\n//   crawlerOptions: sc.crawlerOptions,\n//   scrapeOptions: sc.scrapeOptions,\n//   internalOptions: sc.internalOptions,\n//   origin: \"index\",\n//   crawl_id: id,\n//   webhook: null,\n//   v1: true,\n// }, {}, crypto.randomUUID(), 50);\n\n// we restructure and make all of the arrays we need to fill into objects,\n// adding them to a single object so the llm can fill them one at a time\n// TODO: make this work for more complex schemas where arrays are not first level\n\n// let schemasForLLM: {} = {};\n// for (const key in largeArraysSchema) {\n//   const originalSchema = structuredClone(largeArraysSchema[key].items);\n//   console.log(\n//     \"key\",\n//     key,\n//     \"\\noriginalSchema\",\n//     JSON.stringify(largeArraysSchema[key], null, 2),\n//   );\n//   let clonedObj = {\n//     type: \"object\",\n//     properties: {\n//       informationFilled: {\n//         type: \"boolean\",\n//       },\n//       data: {\n//         type: \"object\",\n//         properties: originalSchema.properties,\n//       },\n//     },\n//   };\n//   schemasForLLM[key] = clonedObj;\n// }\n"]}