{"version":3,"file":"completions.js","sourceRoot":"","sources":["../../../../src/lib/extract/completions.ts"],"names":[],"mappings":";AAAA,4BAA4B;;AAE5B,+BAA+B;AAC/B,uDAAuD;AACvD,kDAAkD;AAClD,+DAA+D;AAC/D,0CAA0C;AAC1C,2BAA2B;AAE3B,2BAA2B;AAC3B,sBAAsB;AAEtB,+CAA+C;AAC/C,mCAAmC;AACnC,6DAA6D;AAC7D,qCAAqC;AACrC,MAAM;AACN,IAAI;AAEJ,wCAAwC;AACxC,2BAA2B;AAC3B,qBAAqB;AACrB,kBAAkB;AAClB,0BAA0B;AAC1B,IAAI;AAEJ,kEAAkE;AAClE,iCAAiC;AACjC,iCAAiC;AACjC,kEAAkE;AAElE,8DAA8D;AAC9D,aAAa;AACb,qDAAqD;AACrD,QAAQ;AAER,kDAAkD;AAClD,IAAI;AAEJ,kDAAkD;AAClD,kBAAkB;AAClB,kBAAkB;AAClB,YAAY;AACZ,YAAY;AACZ,2CAA2C;AAC3C,qBAAqB;AACrB,sDAAsD;AACtD,OAAO;AACP,iCAAiC;AACjC,iCAAiC;AACjC,kEAAkE;AAElE,0CAA0C;AAC1C,uBAAuB;AACvB,sBAAsB;AAEtB,+CAA+C;AAC/C,UAAU;AACV,wDAAwD;AACxD,iCAAiC;AACjC,sBAAsB;AACtB,4EAA4E;AAC5E,yLAAyL;AACzL,gBAAgB;AAChB,sBAAsB;AACtB,MAAM;AAEN,iCAAiC;AACjC,4EAA4E;AAC5E,4KAA4K;AAC5K,MAAM;AAEN,8EAA8E;AAC9E,iBAAiB;AACjB,wBAAwB;AACxB,sBAAsB;AACtB,yBAAyB;AACzB,WAAW;AACX,6BAA6B;AAC7B,qCAAqC;AACrC,SAAS;AACT,yBAAyB;AACzB,2CAA2C;AAC3C,wDAAwD;AACxD,MAAM;AAEN,sEAAsE;AACtE,sBAAsB;AACtB,aAAa;AACb,kBAAkB;AAClB,yDAAyD;AACzD,gFAAgF;AAChF,UAAU;AACV,wBAAwB;AACxB,0BAA0B;AAC1B,uHAAuH;AACvH,0EAA0E;AAC1E,WAAW;AACX,SAAS;AACT,8BAA8B;AAC9B,YAAY;AACZ,iCAAiC;AACjC,2BAA2B;AAC3B,sCAAsC;AACtC,8BAA8B;AAC9B,4BAA4B;AAC5B,eAAe;AACf,YAAY;AACZ,mCAAmC;AACnC,QAAQ;AAER,yHAAyH;AACzH,4EAA4E;AAC5E,MAAM;AAEN,iEAAiE;AACjE,aAAa;AACb,iCAAiC;AACjC,kBAAkB;AAClB,mBAAmB;AACnB,iBAAiB;AACjB,SAAS;AACT,OAAO;AACP,IAAI","sourcesContent":["// use llmExtract.ts instead\n\n// import OpenAI from \"openai\";\n// import { encoding_for_model } from \"@dqbd/tiktoken\";\n// import { TiktokenModel } from \"@dqbd/tiktoken\";\n// import { ExtractOptions } from \"../../controllers/v1/types\";\n// import { Document } from \"../entities\";\n// import { z } from \"zod\";\n\n// const maxTokens = 32000;\n// const modifier = 4;\n\n// export class LLMRefusalError extends Error {\n//   constructor(refusal: string) {\n//     super(\"LLM refused to extract the website's content\");\n//     this.name = \"LLMRefusalError\";\n//   }\n// }\n\n// interface GenerateCompletionsParams {\n//   systemPrompt?: string;\n//   prompt?: string;\n//   schema?: any;\n//   pagesContent: string;\n// }\n\n// export async function generateBasicCompletion(prompt: string) {\n//   const openai = new OpenAI();\n//   const model: TiktokenModel =\n//     (process.env.MODEL_NAME as TiktokenModel) || \"gpt-4o-mini\";\n\n//   const completion = await openai.chat.completions.create({\n//     model,\n//     messages: [{ role: \"user\", content: prompt }],\n//   });\n\n//   return completion.choices[0].message.content;\n// }\n\n// export async function generateFinalExtraction({\n//   pagesContent,\n//   systemPrompt,\n//   prompt,\n//   schema,\n// }: GenerateCompletionsParams): Promise<{\n//   content: string;\n//   metadata: { numTokens: number; warning: string };\n// }> {\n//   const openai = new OpenAI();\n//   const model: TiktokenModel =\n//     (process.env.MODEL_NAME as TiktokenModel) || \"gpt-4o-mini\";\n\n//   let extractionContent = pagesContent;\n//   let numTokens = 0;\n//   let warning = \"\";\n\n//   const encoder = encoding_for_model(model);\n//   try {\n//     const tokens = encoder.encode(extractionContent);\n//     numTokens = tokens.length;\n//   } catch (error) {\n//     extractionContent = extractionContent.slice(0, maxTokens * modifier);\n//     warning = `Failed to derive number of LLM tokens the extraction might use -- the input has been automatically trimmed to the maximum number of tokens (${maxTokens}) we support.`;\n//   } finally {\n//     encoder.free();\n//   }\n\n//   if (numTokens > maxTokens) {\n//     extractionContent = extractionContent.slice(0, maxTokens * modifier);\n//     warning = `The extraction content would have used more tokens (${numTokens}) than the maximum we allow (${maxTokens}). -- the input has been automatically trimmed.`;\n//   }\n\n//   if (schema && (schema.type === \"array\" || schema._type === \"ZodArray\")) {\n//     schema = {\n//       type: \"object\",\n//       properties: {\n//         items: schema,\n//       },\n//       required: [\"items\"],\n//       additionalProperties: false,\n//     };\n//   } else if (schema) {\n//     schema.additionalProperties = false;\n//     schema.required = Object.keys(schema.properties);\n//   }\n\n//   const jsonCompletion = await openai.beta.chat.completions.parse({\n//     temperature: 0,\n//     model,\n//     messages: [\n//       { role: \"system\", content: systemPrompt ?? \"\" },\n//       { role: \"user\", content: [{ type: \"text\", text: extractionContent }] },\n//       {\n//         role: \"user\",\n//         content: prompt\n//           ? `Transform the above content into structured JSON output based on the following user request: ${prompt}`\n//           : \"Transform the above content into structured JSON output.\",\n//       },\n//     ],\n//     response_format: schema\n//       ? {\n//           type: \"json_schema\",\n//           json_schema: {\n//             name: \"websiteContent\",\n//             schema: schema,\n//             strict: true,\n//           },\n//         }\n//       : { type: \"json_object\" },\n//   });\n\n//   if (jsonCompletion.choices[0].message.refusal !== null && jsonCompletion.choices[0].message.refusal !== undefined) {\n//     throw new LLMRefusalError(jsonCompletion.choices[0].message.refusal);\n//   }\n\n//   const extraction = jsonCompletion.choices[0].message.parsed;\n//   return {\n//     content: extraction ?? \"\",\n//     metadata: {\n//       numTokens,\n//       warning,\n//     },\n//   };\n// }\n"]}